{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "758b54f0534c440289eb030e5a536ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46e850d3124843eebf7c1c52271ca048"
            ],
            "layout": "IPY_MODEL_5b7fb31d033f45acbe7faef40e47d82e"
          }
        },
        "32ebb66fd75f48488fdd4475c5ede9b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a559df394c0435abaa6949793a3b149",
            "placeholder": "​",
            "style": "IPY_MODEL_6c271894235d4cd5b7c3bba5983c1919",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "238b137187754ef58e46b3d910768468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_052e21308eb24a638b4306c4cdae2048",
            "placeholder": "​",
            "style": "IPY_MODEL_0959915d03114340a8653d1e0d1feaf1",
            "value": ""
          }
        },
        "03a5f0e9440f43bbb26f03701788c280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_1161ac58b66e44f6a2f12895cf7b0605",
            "style": "IPY_MODEL_610335ff899f42ef893c96e05dcdcea1",
            "value": true
          }
        },
        "61ad9df46e2e41789aa80cb00bac6d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_deb2c5c7ed124ff4ade6ad3359c9e31b",
            "style": "IPY_MODEL_d29ca80dffaf4445885830f2c41d4540",
            "tooltip": ""
          }
        },
        "c87b56c2bb5f486d89fcc986735fb11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e624b63cda65497b9d2a19e92ed8692d",
            "placeholder": "​",
            "style": "IPY_MODEL_c4e1f2e856494552b19e7bcb9dc150ba",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "5b7fb31d033f45acbe7faef40e47d82e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "5a559df394c0435abaa6949793a3b149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c271894235d4cd5b7c3bba5983c1919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "052e21308eb24a638b4306c4cdae2048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0959915d03114340a8653d1e0d1feaf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1161ac58b66e44f6a2f12895cf7b0605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "610335ff899f42ef893c96e05dcdcea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "deb2c5c7ed124ff4ade6ad3359c9e31b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d29ca80dffaf4445885830f2c41d4540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e624b63cda65497b9d2a19e92ed8692d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4e1f2e856494552b19e7bcb9dc150ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bca65ae007842269a8c15367ae3f4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_308d725124f74a0faff3d64551a66e45",
            "placeholder": "​",
            "style": "IPY_MODEL_d0b20284879e4eaca3169c34e3de772d",
            "value": "Connecting..."
          }
        },
        "308d725124f74a0faff3d64551a66e45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0b20284879e4eaca3169c34e3de772d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46e850d3124843eebf7c1c52271ca048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2047cb0900840049de1f5dfc462c8fd",
            "placeholder": "​",
            "style": "IPY_MODEL_dbe39ff5e5154ee7bea7539de1d77cc3",
            "value": "Invalid user token. If you didn't pass a user token, make sure you are properly logged in by executing `huggingface-cli login`, and if you did pass a user token, double-check it's correct."
          }
        },
        "d2047cb0900840049de1f5dfc462c8fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe39ff5e5154ee7bea7539de1d77cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bb2fc1811954a699026f29437835b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2afebc098fc14eddb423641681a1ba1e",
              "IPY_MODEL_58e96283060b4d81a5ab18735a0b66da",
              "IPY_MODEL_1cd2a54eacc64c0aa23bc216f3e2c703"
            ],
            "layout": "IPY_MODEL_793db1e1e957457eba570cc31dfea0a2"
          }
        },
        "2afebc098fc14eddb423641681a1ba1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc5bafad029a4c00bc9265b7fe3e72ca",
            "placeholder": "​",
            "style": "IPY_MODEL_f2368d385e5a4ce9a1911f5e29c9ad44",
            "value": "Mistral-7B-Instruct-v0.3.Q2_K.gguf: 100%"
          }
        },
        "58e96283060b4d81a5ab18735a0b66da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c51b1297756416a8f8289e0e266fa93",
            "max": 2722877600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b80561f21d7d4b0daa9ad299127a2483",
            "value": 2722877600
          }
        },
        "1cd2a54eacc64c0aa23bc216f3e2c703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eefde41551fc4738a30d630d89906de0",
            "placeholder": "​",
            "style": "IPY_MODEL_89668a19e87b48478ab37b4b8952dc88",
            "value": " 2.72G/2.72G [01:06&lt;00:00, 42.7MB/s]"
          }
        },
        "793db1e1e957457eba570cc31dfea0a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc5bafad029a4c00bc9265b7fe3e72ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2368d385e5a4ce9a1911f5e29c9ad44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c51b1297756416a8f8289e0e266fa93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b80561f21d7d4b0daa9ad299127a2483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eefde41551fc4738a30d630d89906de0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89668a19e87b48478ab37b4b8952dc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/Deffro/Data-Science-Portfolio/tree/master\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17ATJVumI5cBGmQjkVsN1gk-Gn3-_BMuP?usp=sharing)"
      ],
      "metadata": {
        "id": "UQXvxmoMmjto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unlocking the Potential of Generative AI: Advanced Prompt Engineering and Model Optimization**\n",
        "\n",
        "## **Introduction**\n",
        "\n",
        "Generative AI is transforming industries by enabling machines to produce human-like text, generate structured outputs, and even creatively solve complex problems.\n",
        "\n",
        "However, the real magic happens when you combine cutting-edge models with advanced techniques like **prompt engineering**, **quantized model optimization**, and **grammar-constrained sampling**.\n",
        "\n",
        "In this project, I explore the forefront of these techniques, demonstrating how to harness the capabilities of state-of-the-art language models to achieve specific and efficient results.\n",
        "\n",
        "From crafting intricate prompts to controlling the randomness and structure of outputs, this work reflects the fusion of creativity, technical expertise, and a deep understanding of model behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## **What This Project Explores**\n",
        "\n",
        "This project focuses on three pivotal aspects of generative AI:\n",
        "\n",
        "### **1. Advanced Prompt Engineering**\n",
        "- Learn how to guide models using precise instructions, multi-component prompts, and **in-context learning** (zero-shot, one-shot, and few-shot prompting).\n",
        "- Use advanced prompt strategies to create structured outputs like **JSON-based descriptions**.\n",
        "\n",
        "### **2. Optimizing Models with Quantization**\n",
        "- Explore how **quantized models** enable high-performance inference with reduced memory and computational requirements.\n",
        "- Understand the trade-offs between file size, precision, and model accuracy using quantization levels such as Q2, Q4, and fp16.\n",
        "\n",
        "### **3. Ensuring Structured Outputs with Grammar Constraints**\n",
        "- Enforce predefined formats like JSON during the token generation process using **grammar-constrained sampling**.\n",
        "- Apply these techniques to achieve reliable and application-ready outputs in domains like structured data generation and sentiment classification.\n",
        "\n",
        "---\n",
        "\n",
        "## **Why It Matters**\n",
        "\n",
        "As AI continues to integrate into our lives, the ability to guide and optimize these systems has become a vital skill. This project showcases the intersection of cutting-edge AI techniques and real-world applications, empowering anyone to:\n",
        "\n",
        "- Efficiently deploy models on resource-constrained devices.\n",
        "- Achieve creative and structured outputs for diverse use cases.\n",
        "- Build confidence in AI outputs through rigorous validation and control.\n"
      ],
      "metadata": {
        "id": "yhZ8rrJsRofZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers>=4.40.1\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\n",
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "YH0HL5geeyqI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load a Text Generation Model**"
      ],
      "metadata": {
        "id": "H3jO1NR5a96X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"border-left: 5px solid #2196F3; background-color: #f1f9ff; padding: 10px 15px; font-family: Arial, sans-serif; font-size: 14px; line-height: 1.5; color: #333;\">\n",
        "  <strong style=\"color: #0d47a1; font-size: 16px;\">Tip:</strong>\n",
        "  Some Hugging Face models need access to use. In case you don't have a Hugging Face account with an access token, you should use another model.\n",
        "  <br><br>\n",
        "  In that case, replace <code style=\"background-color: #e8f4fc; padding: 2px 4px; border-radius: 4px;\">openai-community/gpt2</code> with <code style=\"background-color: #e8f4fc; padding: 2px 4px; border-radius: 4px;\">microsoft/Phi-3-mini-4k-instruct</code>.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "E9rTCPJKsxXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Launch the login widget\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64,
          "referenced_widgets": [
            "758b54f0534c440289eb030e5a536ebf",
            "32ebb66fd75f48488fdd4475c5ede9b1",
            "238b137187754ef58e46b3d910768468",
            "03a5f0e9440f43bbb26f03701788c280",
            "61ad9df46e2e41789aa80cb00bac6d92",
            "c87b56c2bb5f486d89fcc986735fb11b",
            "5b7fb31d033f45acbe7faef40e47d82e",
            "5a559df394c0435abaa6949793a3b149",
            "6c271894235d4cd5b7c3bba5983c1919",
            "052e21308eb24a638b4306c4cdae2048",
            "0959915d03114340a8653d1e0d1feaf1",
            "1161ac58b66e44f6a2f12895cf7b0605",
            "610335ff899f42ef893c96e05dcdcea1",
            "deb2c5c7ed124ff4ade6ad3359c9e31b",
            "d29ca80dffaf4445885830f2c41d4540",
            "e624b63cda65497b9d2a19e92ed8692d",
            "c4e1f2e856494552b19e7bcb9dc150ba",
            "2bca65ae007842269a8c15367ae3f4a7",
            "308d725124f74a0faff3d64551a66e45",
            "d0b20284879e4eaca3169c34e3de772d",
            "46e850d3124843eebf7c1c52271ca048",
            "d2047cb0900840049de1f5dfc462c8fd",
            "dbe39ff5e5154ee7bea7539de1d77cc3"
          ]
        },
        "id": "6QIN8h_PoAqm",
        "outputId": "cfb52ea8-987f-47e9-9b42-b2535579fc1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "758b54f0534c440289eb030e5a536ebf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=False,\n",
        ")"
      ],
      "metadata": {
        "id": "gF3D8p26mkbv",
        "outputId": "9bcaa464-45ae-4420-a5e4-df761e17161a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary of Parameters**\n",
        "\n",
        "| **Parameter**         | **Value**                     | **Description**                                                                 |\n",
        "|------------------------|-------------------------------|---------------------------------------------------------------------------------|\n",
        "| **task**              | `\"text-generation\"`           | Defines the task type: generates text continuations based on input prompts.     |\n",
        "| **model**             | `\"Qwen/Qwen2.5-1.5B-Instruct\"`| Instruction-tuned generative model with 1.5B parameters, optimized for NLP tasks. |\n",
        "| **`return_full_text`**| `False`                       | Excludes the input prompt from the output, returning only the generated text.   |\n",
        "| **`max_new_tokens`**  | `100`                         | Limits the number of tokens generated to control output length and cost.        |\n",
        "| **`do_sample`**       | `False`                       | Disables randomness to ensure deterministic, reproducible outputs.              |\n"
      ],
      "metadata": {
        "id": "nfktwdh6qTyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Using a simple prompt**"
      ],
      "metadata": {
        "id": "66qfeb2gbGUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Given the company Tesla, describe the best pokemon it would represent. Write a short description of the pokemon-company.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "9PR5bMv3ZiWF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply prompt template\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "prompt"
      ],
      "metadata": {
        "id": "n_AdeU8mngDG",
        "outputId": "a9823699-81c3-4697-dc69-7643bc750f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGiven the company Tesla, describe the best pokemon it would represent. Write a short description of the pokemon-company.<|im_end|>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Should `messages` Have `role` and `content`?\n",
        "\n",
        "The `messages` structure with `role` and `content` fields is commonly used for **chat-style conversational models**. Below are the key reasons why this format is important and beneficial:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Mimics Multi-Turn Conversations**\n",
        "- The **`role`** field (e.g., `user`, `assistant`, or `system`) helps the model understand the context of the conversation:\n",
        "  - **`user`**: Represents the human's input or query.\n",
        "  - **`assistant`**: Represents the AI model's response.\n",
        "  - **`system`**: Provides instructions or context for the AI’s behavior (if supported by the model).\n",
        "- This structure enables models to manage multi-turn conversations effectively by maintaining a clear distinction between different speakers.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Instruction-Following Models**\n",
        "- Instruction-tuned models (e.g., Qwen, Flan-T5, or ChatGPT-like models) are often trained on datasets where prompts are structured this way.\n",
        "- Including `role` and `content` aligns with the model's training data, improving its ability to:\n",
        "  - Interpret the input correctly.\n",
        "  - Generate coherent and contextually relevant responses.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Flexibility for Chat Applications**\n",
        "- The `messages` format allows for more complex and dynamic interactions, such as:\n",
        "  - Handling multiple speakers in a conversation.\n",
        "  - Maintaining context across multi-turn dialogues.\n",
        "  - Incorporating dynamic instructions via the `system` role.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Model-Specific Requirements**\n",
        "- Some chat-focused models (like OpenAI’s ChatGPT, GPT-4, or Qwen-Chat) expect input in the `messages` format.\n",
        "- Omitting `role` and `content` for these models might lead to:\n",
        "  - Errors in processing.\n",
        "  - Misinterpretation of the input, resulting in lower-quality responses.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Generality and Consistency**\n",
        "- Using `role` and `content` creates a standardized format that works across different chat-focused models.\n",
        "- It ensures compatibility, making it easier to switch models or integrate them into applications.\n",
        "\n",
        "---\n",
        "\n",
        "### **What If the Model Does Not Support `messages`?**\n",
        "\n",
        "If the model you’re using does not natively support the `messages` structure:\n",
        "\n",
        "1. **Simplify the Input**:\n",
        "   - Convert `messages` into a plain text prompt:\n",
        "     ```python\n",
        "     prompt = \"user: Given the company Tesla, describe the best Pokemon it would represent. Write a short description of the Pokemon-company.\"\n",
        "     ```\n",
        "   - Pass the `prompt` to the pipeline:\n",
        "     ```python\n",
        "     output = pipe(prompt)\n",
        "     print(output[0][\"generated_text\"])\n",
        "     ```\n",
        "\n",
        "2. **Use a Chat-Compatible Model**:\n",
        "   - Models designed for chat (e.g., `Qwen-Chat`) will handle the `messages` format directly.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaway**\n",
        "The `messages` format is **not mandatory** for all models but is critical for:\n",
        "- Models trained on conversational or instruction-following datasets.\n",
        "- Applications requiring multi-turn chat or contextual interactions.\n",
        "\n",
        "If the model does not support `messages`, convert it into a single string prompt to ensure compatibility.\n"
      ],
      "metadata": {
        "id": "-JRYeLYOpdC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the output\n",
        "output = pipe(messages)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "dbNd18XarVz6",
        "outputId": "3c43acae-5336-4bb5-db47-b9dbd77d628d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla could potentially be represented by the Pokémon \"Gyarados.\" Gyarados is known for its massive size and powerful attacks, which align with Tesla's focus on innovation and technological advancements.\n",
            "\n",
            "**Description:**\n",
            "In the world of Pokémon, Gyarados stands as an iconic symbol of Tesla's innovative spirit. This colossal water-type Pokémon boasts a sleek, streamlined body that resembles a futuristic submarine, reflecting Tesla's commitment to cutting-edge technology in all aspects of life. Its large fins and sharp teeth suggest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the `Qwen/Qwen2.5-1.5B-Instruct` model, given a simple prompt generated an \"ok\" response.\n",
        "\n",
        "But there is an important flaw. The generated text is cut off as per our `max_new_tokens=100` and `do_sample=False` configuration.\n",
        "\n",
        "We can allow the model to generate more tokens to complete the thought.\n",
        "\n"
      ],
      "metadata": {
        "id": "stJMqZlHqRyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Prompt engineering with an advanced prompt**"
      ],
      "metadata": {
        "id": "wXrRDPfMbQq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Components of Prompts\n",
        "For more nuanced tasks, consider incorporating the following:\n",
        "\n",
        "1. **Persona**: Define a role for the LLM. For example, “You are a Pokemon expert and a financial analyst.”\n",
        "2. **Instruction**: Specify the task clearly. Example: “Describe a company as a Pokemon.”\n",
        "3. **Context**: Provide background or additional information. Example: “Focus on Tesla’s innovation.”\n",
        "4. **Format**: Specify the structure of the output. Example: “Output the response in JSON format.”\n",
        "5. **Audience**: Tailor the tone or complexity to the intended audience. Example: “The output is for investors.”\n",
        "6. **Tone**: Adjust the language style. Example: “Use a playful tone.”\n"
      ],
      "metadata": {
        "id": "XE_UD2LRsu95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt components\n",
        "persona = \"You are a Pokemon expert and a financial analyst in the stock market.\\n\"\n",
        "instruction = \"Given a company name, you will describe the best pokemon it would represent.\\n\"\n",
        "context = \"Write a short description of the pokemon-company.\\n\"\n",
        "data_format = \"Give the output in a json format.\\n\"\n",
        "audience = \"The output is designed for investors.\\n\"\n",
        "tone = \"The tone should be playful.\\n\"\n",
        "text = \"Tesla\"\n",
        "data = f\"Company: {text}\"\n",
        "\n",
        "# The full prompt - remove and add pieces to view its impact on the generated output\n",
        "query = persona + instruction + context + data_format + audience + tone + data\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "]\n",
        "\n",
        "print(pipe.tokenizer.apply_chat_template(messages, tokenize=False))"
      ],
      "metadata": {
        "id": "sy43Rg5Ov1Xc",
        "outputId": "64a91671-56a5-42e3-e2ef-506d9f17a89c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "You are a Pokemon expert and a financial analyst in the stock market.\n",
            "Given a company name, you will describe the best pokemon it would represent.\n",
            "Write a short description of the pokemon-company.\n",
            "Give the output in a json format.\n",
            "The output is designed for investors.\n",
            "The tone should be playful.\n",
            "Company: Tesla<|im_end|>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(messages)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "e57wpyFS0ZrX",
        "outputId": "91d44aa3-add6-4b41-aad0-ca2f16236b1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"pokemon_company\": {\n",
            "    \"name\": \"Eevee\",\n",
            "    \"description\": \"Tesla is like an Eevee - versatile and adaptable, always evolving to meet new challenges. Just as Eevee can transform into various forms based on its surroundings, Tesla has transformed from an electric car pioneer into a leader in renewable energy solutions. Its journey continues with innovations that push boundaries and redefine what's possible.\"\n",
            "  }\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our Qwen model created a perfect json just as we instructed it."
      ],
      "metadata": {
        "id": "aPtx7iqEtNQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Adjusting Temperature and Top-p in Text Generation**"
      ],
      "metadata": {
        "id": "nuxdMgwuu4Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "When working with text-generation models, **temperature** and **top-p (nucleus sampling)** are key parameters that control the diversity and creativity of the model's output. Fine-tuning these parameters allows you to balance randomness and coherence, tailoring the model’s response to your specific needs.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What Is Temperature?**\n",
        "\n",
        "- **Definition**:\n",
        "  - Temperature is a parameter that adjusts the randomness of the model's predictions.\n",
        "  - It scales the probabilities of the next word/token being chosen, influencing how deterministic or creative the output is.\n",
        "\n",
        "- **How It Works**:\n",
        "  - Lower temperatures make the output more deterministic.\n",
        "  - Higher temperatures introduce more randomness by flattening the probability distribution.\n",
        "\n",
        "- **Examples**:\n",
        "  - **Low Temperature (e.g., 0.2)**:\n",
        "    - Focuses on the most probable tokens, generating predictable and focused text.\n",
        "    - Suitable for tasks like factual summarization or code generation.\n",
        "  - **High Temperature (e.g., 1.0)**:\n",
        "    - Expands the range of token selection, leading to more diverse and creative outputs.\n",
        "    - Ideal for storytelling, poetry, or open-ended tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. What Is Top-p (Nucleus Sampling)?**\n",
        "\n",
        "- **Definition**:\n",
        "  - Top-p controls the cumulative probability of tokens considered for selection.\n",
        "  - It limits the model to only consider the most probable tokens whose cumulative probability falls below \\( p \\).\n",
        "\n",
        "- **How It Works**:\n",
        "  - The model generates tokens by sampling from a dynamically adjusted subset of all possible tokens.\n",
        "  - The size of the subset depends on the cumulative probability \\( p \\).\n",
        "\n",
        "- **Examples**:\n",
        "  - **Low Top-p (e.g., 0.2)**:\n",
        "    - Restricts the selection to only the top 20% of tokens with the highest probabilities.\n",
        "    - Produces focused and coherent outputs.\n",
        "  - **High Top-p (e.g., 0.9)**:\n",
        "    - Expands the range of potential tokens, allowing for more diverse outputs.\n",
        "    - Balances creativity and coherence.\n",
        "\n",
        "- **How It Differs from Temperature**:\n",
        "  - While temperature scales probabilities globally, top-p selects a subset of tokens based on their cumulative probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Using Temperature and Top-p Together**\n",
        "\n",
        "- These parameters can be used independently or combined for better control:\n",
        "  - **High Temperature + High Top-p**: Encourages maximum creativity, often at the cost of coherence.\n",
        "  - **Low Temperature + Low Top-p**: Ensures deterministic and precise outputs.\n",
        "  - **High Temperature + Low Top-p**: Adds creativity but restricts it to highly probable tokens for a balance of diversity and accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Choosing the Right Settings**\n",
        "\n",
        "### **Task-Specific Suggestions**:\n",
        "| **Task**                | **Temperature** | **Top-p** |\n",
        "|--------------------------|-----------------|-----------|\n",
        "| Factual Summarization    | 0.2–0.5         | 0.9–1.0   |\n",
        "| Creative Writing         | 0.8–1.2         | 0.8–1.0   |\n",
        "| Code Generation          | 0.1–0.3         | 0.9–1.0   |\n",
        "| Open-Ended Dialogues     | 0.7–1.0         | 0.8–1.0   |\n",
        "\n",
        "### **Experimentation Is Key**:\n",
        "- Start with defaults: Temperature = **1.0**, Top-p = **0.9**.\n",
        "- Adjust incrementally to see how the output changes.\n",
        "- Combine settings for the desired balance of creativity and accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Example Code**\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a text generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"gpt2\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,  # Adjust randomness\n",
        "    top_p=0.9,        # Use nucleus sampling\n",
        "    do_sample=True    # Enable sampling\n",
        ")\n",
        "\n",
        "# Generate text\n",
        "output = pipe(\"Once upon a time, in a land far away, there was a\", num_return_sequences=1)\n",
        "print(output[0][\"generated_text\"])\n",
        "```\n",
        "\n",
        "You can override the default parameters of an already defined pipeline by passing them as arguments during inference."
      ],
      "metadata": {
        "id": "3pCpdtLpba77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a high temperature\n",
        "output = pipe(messages, do_sample=True, temperature=0.7)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "DLcC9xm80gRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe9ba868-37f8-4edd-90f5-437d407e3aa3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Pokemon\": {\n",
            "    \"Name\": \"Steelix\",\n",
            "    \"Description\": \"A powerful and versatile electrician who can handle any situation with ease. Just like Tesla, this company has the ability to adapt to new technologies and stay ahead of the curve.\",\n",
            "    \"Strengths\": [\n",
            "      \"High Voltage Power - The strength of Tesla's batteries powering their cars and energy solutions.\",\n",
            "      \"Innovative Thinking - Steelix constantly looks for ways to improve and innovate, much like how\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a high top_p\n",
        "output = pipe(messages, do_sample=True, top_p=1)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-FLkFPuY-a8",
        "outputId": "f4200ac8-40ea-486c-ceaa-8427b561a634"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"pokemonCompany\": {\n",
            "    \"name\": \"Pikachu\",\n",
            "    \"description\": \"As a financial powerhouse, Tesla represents the electric charge behind its innovative technology. Just like Pikachu's ability to rapidly generate electricity, Tesla’s cutting-edge electric vehicles and renewable energy solutions propel the world forward with speed and efficiency.\",\n",
            "    \"traits\": [\n",
            "      \"Fast-paced growth\",\n",
            "      \"Innovation at the forefront\",\n",
            "      \"Evolving products and services\"\n",
            "    ],\n",
            "    \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. In-Context Learning: Guiding LLMs with Examples**"
      ],
      "metadata": {
        "id": "KtYQLGjGatrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While detailed descriptions and instructions are essential for crafting effective prompts, there's another powerful tool in prompt engineering: **examples**.\n",
        "\n",
        "By showing the model exactly what we want it to accomplish, we can achieve more precise and reliable outputs.\n",
        "\n",
        "This technique, often called **in-context learning**, allows the model to learn from the examples within the same prompt, without requiring additional fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## **What Is In-Context Learning?**\n",
        "In-context learning involves providing examples of the task directly within the prompt.\n",
        "\n",
        "These examples demonstrate the structure, tone, and content of the desired output, making it easier for the model to understand and replicate.\n",
        "\n",
        "There are three main types of in-context learning:\n",
        "1. **Zero-Shot Prompting**: No examples are provided; the model must rely entirely on the instruction.\n",
        "2. **One-Shot Prompting**: A single example is included in the prompt.\n",
        "3. **Few-Shot Prompting**: Multiple examples are used to guide the model.\n",
        "\n",
        "Each method has its strengths:\n",
        "- **Zero-Shot**: Works well for straightforward tasks with clear instructions.\n",
        "- **One-Shot/Few-Shot**: Improves performance for more complex tasks by reducing ambiguity and providing context.\n",
        "\n",
        "---\n",
        "\n",
        "## **Why Use Examples?**\n",
        "Examples allow you to:\n",
        "- **Clarify Ambiguity**: Show the model the exact format and type of output you expect.\n",
        "- **Reduce Errors**: By replicating the example, the model is less likely to deviate from the intended task.\n",
        "- **Demonstrate Style**: Examples help convey tone, style, and complexity of language for creative or domain-specific tasks.\n"
      ],
      "metadata": {
        "id": "nY1ivWyAbucn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-shot learning: Providing an example of the output structure\n",
        "instruction = \"\"\"Given a company name, you will describe the best pokemon it would represent. Use exactly this format:\n",
        "\n",
        "{\n",
        "  \"company\": \"The provided company name\",\n",
        "  \"pokemon\": \"The best pokemon it represents from the first generation\",\n",
        "  \"description\": \"A short description in 40 characters\",\n",
        "  \"type\": \"The pokemon type\",\n",
        "  \"moves\": {\n",
        "    \"move_1\": \"the first company move that should be relevant to the company\",\n",
        "    \"move_2\": \"the second company move that should be relevant to the company\",\n",
        "    \"move_3\": \"the third company move that should be relevant to the company\",\n",
        "    \"move_4\": \"the forth company move that should be relevant to the company\",\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "text = \"Tesla\"\n",
        "data = f\"Company: {text}\"\n",
        "\n",
        "query = instruction + data\n",
        "\n",
        "one_shot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": query}\n",
        "]\n",
        "\n",
        "# Generate the output\n",
        "outputs = pipe(one_shot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUc0gIdtb2NW",
        "outputId": "723ea8ee-6dca-443a-bbe1-1940b30b6a53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"company\": \"Tesla\",\n",
            "  \"pokemon\": \"Eevee\",\n",
            "  \"description\": \"Fast and versatile.\",\n",
            "  \"type\": \"Normal/Flying\",\n",
            "  \"moves\": {\n",
            "    \"move_1\": \"Quick Attack\",\n",
            "    \"move_2\": \"Fly\",\n",
            "    \"move_3\": \"Thunderbolt\",\n",
            "    \"move_4\": \"Psychic\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Forcing JSON output**"
      ],
      "metadata": {
        "id": "fuCxXNfGec-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with Large Language Models (LLMs), one challenge is ensuring the generated output adheres to specific formats or rules.\n",
        "\n",
        "Even with carefully crafted prompts, models might occasionally generate undesired or invalid results.\n",
        "\n",
        "### **What is Grammar-Constrained Sampling?**\n",
        "\n",
        "Grammar-constrained sampling ensures that the output of an LLM adheres to predefined formats or rules. Unlike regular prompting or few-shot learning, where we guide the model with examples or instructions, this method validates and enforces output structures during generation.\n",
        "\n",
        "#### **Key Benefits**:\n",
        "1. **Output Validation**: Ensures adherence to specific formats, such as JSON or XML.\n",
        "2. **Enhanced Control**: Prevents models from generating irrelevant or invalid tokens.\n",
        "3. **Improved Reliability**: Reduces post-processing errors in applications that require structured data.\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Works**\n",
        "\n",
        "1. **Defining Rules or Grammar**:\n",
        "   - Predefine constraints (e.g., JSON schema, predefined tokens, or formats).\n",
        "   - Limit the model’s possible outputs during token sampling.\n",
        "\n",
        "2. **Validation During Generation**:\n",
        "   - Instead of validating the output after generation, grammar-constrained sampling applies rules in real-time during token selection.\n",
        "\n",
        "3. **Applications**:\n",
        "   - **Sentiment Classification**: Limit output to predefined labels like `\"positive\"`, `\"neutral\"`, and `\"negative\"`.\n",
        "   - **Structured Data Generation**: Generate JSON, XML, or other structured formats.\n",
        "   - **Domain-Specific Outputs**: Enforce outputs relevant to a specific domain (e.g., RPG character creation, company profiles).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mvtXjZS_NE4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF\",\n",
        "\tfilename=\"Mistral-7B-Instruct-v0.3.Q2_K.gguf\",\n",
        "  n_gpu_layers=-1,\n",
        "  n_ctx=2048,\n",
        "  verbose=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "0bb2fc1811954a699026f29437835b92",
            "2afebc098fc14eddb423641681a1ba1e",
            "58e96283060b4d81a5ab18735a0b66da",
            "1cd2a54eacc64c0aa23bc216f3e2c703",
            "793db1e1e957457eba570cc31dfea0a2",
            "dc5bafad029a4c00bc9265b7fe3e72ca",
            "f2368d385e5a4ce9a1911f5e29c9ad44",
            "5c51b1297756416a8f8289e0e266fa93",
            "b80561f21d7d4b0daa9ad299127a2483",
            "eefde41551fc4738a30d630d89906de0",
            "89668a19e87b48478ab37b4b8952dc88"
          ]
        },
        "id": "J27UdWwpeiph",
        "outputId": "e0e84985-b879-4aa9-8ecd-65d31ceecd8d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Mistral-7B-Instruct-v0.3.Q2_K.gguf:   0%|          | 0.00/2.72G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bb2fc1811954a699026f29437835b92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLaMA and Quantized Models**\n",
        "\n",
        "LLaMA (Large Language Model Meta AI) is a family of large language models designed to be efficient and scalable for various NLP tasks. It provides high performance while requiring fewer computational resources compared to other models of similar size.\n",
        "\n",
        "### **What Are Quantized Models?**\n",
        "Quantization is a process of reducing the precision of a model's weights from 32-bit floating point to lower precision formats (e.g., 8-bit, 4-bit). This dramatically reduces the model's size and improves inference speed with minimal impact on accuracy.\n",
        "\n",
        "### **Advantages of Quantization:**\n",
        "1. **Reduced Memory Footprint**: Smaller models use less storage and memory.\n",
        "2. **Faster Inference**: Quantized models perform computations faster, especially on devices with limited resources.\n",
        "3. **Efficient GPU/CPU Usage**: Reduces computational overhead, making it possible to run larger models on less powerful hardware.\n",
        "\n",
        "---\n",
        "\n",
        "## **Parameters in LLaMA Loading**\n",
        "\n",
        "The following parameters are used when loading a quantized LLaMA model:\n",
        "\n",
        "| **Parameter**     | **Description**                                                                                           |\n",
        "|--------------------|-----------------------------------------------------------------------------------------------------------|\n",
        "| **repo_id**        | The Hugging Face repository where the quantized model resides.                                            |\n",
        "| **filename**       | The specific file containing the quantized model.                                                        |\n",
        "| **n_gpu_layers**   | The number of layers to offload to the GPU. Setting `-1` offloads all layers to the GPU.                  |\n",
        "| **n_ctx**          | The context size or maximum token length that the model can process in one forward pass.                 |\n",
        "| **verbose**        | A boolean flag to enable or disable detailed logging during the model loading process.                    |\n",
        "\n",
        "---\n",
        "\n",
        "## **Quantization Levels and File Sizes**\n",
        "\n",
        "Quantization levels are indicated in the model filenames and determine the precision of the weights:\n",
        "\n",
        "| **Quantization Level** | **Precision**   | **Approx. File Size** | **Description**                                                                                   |\n",
        "|-------------------------|-----------------|------------------------|---------------------------------------------------------------------------------------------------|\n",
        "| **IQ1_M**              | 1-bit (Mixed)  | ~1.76 GB              | Extremely compact with significant trade-offs in accuracy.                                       |\n",
        "| **IQ2_XS**             | 2-bit (Extra Small) | ~2.2 GB           | Compact and suitable for lightweight applications.                                               |\n",
        "| **IQ3_XS**             | 3-bit (Extra Small) | ~3.02 GB          | Balanced between compactness and performance.                                                    |\n",
        "| **IQ4_XS**             | 4-bit (Extra Small) | ~3.91 GB          | Higher precision, suitable for more demanding applications.                                      |\n",
        "| **Q2_K**               | 2-bit (K-bit)  | ~2.72 GB              | Optimized for efficiency while maintaining reasonable performance.                               |\n",
        "| **Q6_K**               | 6-bit (K-bit)  | ~5.95 GB              | High precision, offering excellent accuracy at the cost of larger file size.                    |\n",
        "| **fp16**               | 16-bit (Floating Point) | ~14.5 GB       | Full precision for maximum accuracy, requiring significant computational resources.             |\n",
        "\n",
        "---\n",
        "\n",
        "## **Suffixes and Their Meanings**\n",
        "\n",
        "The model filenames include suffixes that provide additional information about their configuration:\n",
        "\n",
        "| **Suffix**    | **Meaning**                                                                                       |\n",
        "|---------------|---------------------------------------------------------------------------------------------------|\n",
        "| **Q2**        | Indicates 2-bit quantization for model weights.                                                   |\n",
        "| **Q3/Q4**     | Indicates 3-bit or 4-bit quantization, offering progressively higher precision.                   |\n",
        "| **K**         | Refers to specific quantization algorithms (e.g., K-bit quantization).                            |\n",
        "| **XS**        | Extra Small configuration, optimized for minimal resource usage.                                  |\n",
        "| **fp16**      | Full precision (16-bit floating-point), designed for high-accuracy use cases but resource-heavy.  |\n",
        "| **M/S/L**     | Mixed (M), Small (S), or Large (L) configurations, denoting variations in quantization or model structure. |\n"
      ],
      "metadata": {
        "id": "sgbwSrPKQ8_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate output\n",
        "output = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Given the company Tesla, describe the best pokemon it would represent along with its atributes and moves.\"},\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0,\n",
        ")['choices'][0]['message'][\"content\"]\n"
      ],
      "metadata": {
        "id": "ANVRh48LgVsJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Format as json\n",
        "json_output = json.dumps(json.loads(output), indent=4)\n",
        "print(json_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yICSPIKgshj",
        "outputId": "20f7818c-d8bf-49a6-d9c7-bc694e820ad2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"company\": \"Tesla\",\n",
            "    \"pokemon\": \"Voltorb\",\n",
            "    \"attributes\": {\n",
            "        \"type\": [\n",
            "            \"Electric\"\n",
            "        ],\n",
            "        \"hidden_ability\": [\n",
            "            \"Run Away\"\n",
            "        ],\n",
            "        \"stats\": {\n",
            "            \"HP\": 72,\n",
            "            \"Attack\": 60,\n",
            "            \"Defense\": 60,\n",
            "            \"Special Attack\": 60,\n",
            "            \"Special Defense\": 60,\n",
            "            \"Speed\": 90\n",
            "        },\n",
            "        \"evolution\": {\n",
            "            \"evolves_from\": \"Pok\\u00e9mon\",\n",
            "            \"evolution_level\": 22\n",
            "        },\n",
            "        \"moves\": [\n",
            "            \"Tail Glow\",\n",
            "            \"Self-Destruct\",\n",
            "            \"Swift\",\n",
            "            \"Thunder Shock\"\n",
            "        ]\n",
            "    },\n",
            "    \"description\": \"Voltorb, representing Tesla, embodies the company's innovative and forward-thinking spirit. Its Electric type reflects Tesla's focus on electric vehicles. Voltorb's moves, such as Tail Glow and Self-Destruct, symbolize the risk and potential for destruction inherent in innovation and technological advancement. The hidden ability Run Away represents Tesla's resilience and adaptability in the face of challenges.\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}