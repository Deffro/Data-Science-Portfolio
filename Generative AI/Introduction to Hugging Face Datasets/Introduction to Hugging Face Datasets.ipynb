{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe369e2-b767-4cef-bc0a-ebfc8a5dd58e",
   "metadata": {},
   "source": [
    "# What is Hugging Face Datasets?\n",
    "\n",
    "Datasets is one of the three main libraries (Datasets, Transformers, Tokenizers) of the ðŸ¤— Hugging Face ecosystem.\n",
    "Using Datasets we can **access** 101,839 datasets (as of February 2024) and also **publish** and share our own.\n",
    "\n",
    "Using a dataset is the first step of researching and developing our cool AI projects.\n",
    "\n",
    "These datasets are categorized into 6 broad categories called **\"Tasks\"**:\n",
    "\n",
    "![image 1](https://cdn-images-1.medium.com/max/800/1*5e9YFMLDu9OSaTMGycbmIg.png)\n",
    "\n",
    "Each Task is divided into numerous **\"Sub-tasks\"**, totaling over 100. There is a Sub-task for nearly anything.\n",
    "\n",
    "Some examples include *Object Detection, Text Classification, Summarization, Zero-Shot Classification, Question Answering, Text-to-Audio, Time Series Forecasting, and Image-to-3D*.\n",
    "\n",
    "The main goal of ðŸ¤— Datasets is to provide a simple way to load a dataset of any format or type.\n",
    "\n",
    "They surely deliver on this goal, as **reading a dataset takes a single line of code**.\n",
    "\n",
    "In this tutorial, we will explore the four basic properties of the ðŸ¤— Datasets:\n",
    "\n",
    "![image 2](https://cdn-images-1.medium.com/max/800/1*KoPiR26XAAYIoj4FHziwyA.png)\n",
    "\n",
    "# Download a Dataset\n",
    "\n",
    "Before downloading any dataset we have to **install the library**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b2e7a-c4d9-4576-b7fb-49d9a1076059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Hugging Face's transformers and datasets modules\n",
    "# !pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f9f60c35-7d27-4923-a576-3dd2e113e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "\n",
    "from datasets import (\n",
    "    concatenate_datasets,\n",
    "    Dataset,\n",
    "    get_dataset_config_names,\n",
    "    list_datasets,\n",
    "    load_dataset,\n",
    "    load_from_disk,\n",
    "    Value,\n",
    ")\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d217f99e-4411-4daa-9439-4a7d0082327d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 101847 datasets currently available on the Hugging Face Datasets.\n"
     ]
    }
   ],
   "source": [
    "all_datasets = list_datasets()\n",
    "print(f\"There are {len(all_datasets)} datasets currently available on the Hugging Face Datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483eea0-86ea-4941-9eff-09fd8a25976c",
   "metadata": {},
   "source": [
    "Let's first decide which dataset to download.Â \n",
    "\n",
    "The choice is easy for me. Back in 2016, I did my Thesis called \"[Emotion Detection on Movie Reviews](https://deffro.github.io/projects/emotion-detection-on-movie-reviews/)\". I made a brave attempt to construct a classifier capable of classifying a sentence in one of the 6 basic categories of emotion which are anger, disgust, fear, happiness, sadness, and surprise.\n",
    "\n",
    "8 years later, I am ready to revisit this problem since I saw that there is a dataset in ðŸ¤— Datasets called [emotion](https://huggingface.co/datasets/dair-ai/emotion).\n",
    "\n",
    "In this tutorial, I will only load the dataset and I will keep the model training in a future one. Our focus for now is to get familiar with Datasets.\n",
    "\n",
    "Let's **download** the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76a1168c-9551-4921-b063-350b7626279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = load_dataset(\"emotion\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a353b1-15fb-48fb-8f23-e3e192cc0ba5",
   "metadata": {},
   "source": [
    "As downloading might take some time depending on the dataset, you might first want to **inspect it without downloading**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c05c42b-4abc-4645-9e0b-8c57f8e70ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "\n",
    "ds_builder = load_dataset_builder(\"emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78e2ba9e-1a16-4abc-81bf-29d73480ec33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. For more detailed information please refer to the paper.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.info.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e2c944a-3847-4fa5-9a8d-43a503b9b191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0d07333-b5b8-4be3-918d-b931fd5863bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': SplitInfo(name='train', num_bytes=1741533, num_examples=16000, shard_lengths=None, dataset_name='emotion'),\n",
       " 'validation': SplitInfo(name='validation', num_bytes=214695, num_examples=2000, shard_lengths=None, dataset_name='emotion'),\n",
       " 'test': SplitInfo(name='test', num_bytes=217173, num_examples=2000, shard_lengths=None, dataset_name='emotion')}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.info.splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d65e6-3258-47c0-a815-4776d59ac51a",
   "metadata": {},
   "source": [
    "There is also the possibility to **load a single split**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5411699e-83db-4ec6-b5cf-30f72d68365b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(\"emotion\", split=\"train\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cef545-c7da-4454-af3e-14090135d83e",
   "metadata": {},
   "source": [
    "## Meet yourÂ Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c76d4-83b7-432a-bd02-fe9c34c4ebce",
   "metadata": {},
   "source": [
    "Let's see the emotion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "110cae9e-33cb-4897-8c81-8bfa30e9d7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f723e-cd12-4af8-ba70-e5bf2c47d9fe",
   "metadata": {},
   "source": [
    "It looks like a Python dictionary with dataset splits as keys.Â \n",
    "\n",
    "The [Dataset object](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset) is one of the core data structures in ðŸ¤— Datasets. \n",
    "\n",
    "It is based on [Apache Arrow](https://arrow.apache.org/) which is more memory efficient than native Python. It represents data in a columnar format, which is highly efficient for analytical processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3cc2d-4ae3-4580-9726-e31f5f03f37d",
   "metadata": {},
   "source": [
    "Let's see a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a7d3f37-a120-4a6e-82c3-e0bc8217d409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i feel like i have to make the suffering i m seeing mean something',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ffaed-0de4-4570-bacb-9b4451796e08",
   "metadata": {},
   "source": [
    "We can access dataset information in the same way we did using `load_dataset_builder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4393300d-49b6-42d3-acbd-4d38b9b65c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea670491-506a-46c7-9042-6612d2b8ae41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"].num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93685e-ca2f-4a4c-a0a8-6139243253a0",
   "metadata": {},
   "source": [
    "Some datasets contain several sub-datasets. For example, the MInDS-14 dataset has several sub-datasets, each one containing audio data in a different language. These sub-datasets are known as configurations, and you must explicitly select one when loading the dataset. If you donâ€™t provide a configuration name, ðŸ¤— Datasets will raise a ValueError and remind you to choose a configuration.\n",
    "\n",
    "Use the get_dataset_config_names() function to retrieve a list of **all the possible configurations** available to your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a52a59f-0619-4dda-8be7-f38c2cc26a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cs-CZ',\n",
       " 'de-DE',\n",
       " 'en-AU',\n",
       " 'en-GB',\n",
       " 'en-US',\n",
       " 'es-ES',\n",
       " 'fr-FR',\n",
       " 'it-IT',\n",
       " 'ko-KR',\n",
       " 'nl-NL',\n",
       " 'pl-PL',\n",
       " 'pt-PT',\n",
       " 'ru-RU',\n",
       " 'zh-CN',\n",
       " 'all']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_config_names(\"PolyAI/minds14\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e718990-f8f6-431e-b02b-b98ef37cda7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i didnt feel humiliated',\n",
       " 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       " 'im grabbing a minute to post i feel greedy wrong',\n",
       " 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
       " 'i am feeling grouchy']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"][\"text\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48576a8e-905b-49c3-9fb5-aced7afc0844",
   "metadata": {},
   "source": [
    "## Loading your ownÂ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9a69be8-c120-4aeb-b302-29561ac74417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]C:\\Users\\dimit\\miniconda3\\envs\\nlp\\Lib\\site-packages\\datasets\\download\\streaming_download_manager.py:778: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n",
      "Generating train split: 1460 examples [00:00, 31596.33 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'SalePrice'],\n",
       "        num_rows: 1460\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(\"csv\", data_files={\"train\": \"./my_data.csv\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5062d-2ee2-4d14-9d32-ac60491a41a2",
   "metadata": {},
   "source": [
    "You can also load other files types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390650d6-f96a-44fe-86f0-a35119e8313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(\"json\", data_files={'train': 'train.json', 'test': 'test.json'})\n",
    "load_dataset(\"parquet\", data_files={'train': 'train.parquet', 'test': 'test.parquet'})\n",
    "load_dataset(\"arrow\", data_files={'train': 'train.arrow', 'test': 'test.arrow'})\n",
    "Dataset.from_sql(\"data_table_name\", con=\"sqlite:///sqlite_file.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29816b81-01fc-4157-b5e8-545869d8d726",
   "metadata": {},
   "source": [
    "You can also create a Dataset directly from in-memory data structures like Python dictionaries and Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c1869f7-0c70-4c4f-956e-86cfb44e3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\"score\": [10, 7, 8.5]}\n",
    "dataset_d = Dataset.from_dict(my_dict)\n",
    "\n",
    "my_list = [{\"score\": 10}, {\"score\": 7}, {\"score\": 8.5}]\n",
    "dataset_l = Dataset.from_list(my_list)\n",
    "\n",
    "df = pd.DataFrame({\"score\": [10, 7, 8.5]})\n",
    "dataset_p = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f58ac77b-fee5-4709-9619-64520772fca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['score'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49fdba9-a931-4614-a303-4b991a0c5bf4",
   "metadata": {},
   "source": [
    "# Modifying aÂ Dataset\n",
    "![image 3](https://cdn-images-1.medium.com/max/800/1*KDzpnwIUplELboUW9aUv4w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff8799-f6d3-4b0c-8506-6984920ffbd5",
   "metadata": {},
   "source": [
    "Dataset provides functionalities for sorting, shuffling, selecting, filtering, splitting, and sharding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b560584a-122e-44bf-b85b-0dc6c1a4a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting based on the \"text\" column\n",
    "sorted_dataset = emotions[\"train\"].sort(\"text\")  \n",
    "\n",
    "# Provide a seed for reproducibility\n",
    "shuffled_dataset = emotions[\"train\"].shuffle(seed=42)  \n",
    "\n",
    "# Create a new dataset with rows selected following the list/array of indices.\n",
    "selected_dataset = emotions[\"train\"].select(range(5))\n",
    "\n",
    "# Create train, validation, and test sets if your dataset doesnâ€™t already have them.\n",
    "train_dataset, test_dataset = emotions[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Filter the dataset based on a condition\n",
    "filtered_dataset = emotions[\"train\"].filter(lambda example: len(example[\"text\"]))\n",
    "\n",
    "# Sharding is useful for distributing the dataset across multiple processes or nodes\n",
    "sharded_dataset = emotions[\"train\"].shard(num_shards=5, index=0)  # Assuming you have 5 shards and selecting the first one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899f240-dd6c-4421-9166-abd8d89fad5f",
   "metadata": {},
   "source": [
    "You can also **rename** and **remove** columns, **cast** data types, **flatten** nested structures, and get **unique** values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7979bcfb-223c-49c5-b188-c30ee1bf5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a column\n",
    "renamed_dataset = emotions[\"train\"].rename_column(\"text\", \"tweet\")\n",
    "\n",
    "# Remove a column\n",
    "dataset_without_column = emotions[\"train\"].remove_columns([\"text\"])\n",
    "\n",
    "# Cast a column to a different data type\n",
    "dataset_casted = emotions[\"train\"].cast_column(\"label\", Value(\"int8\"))\n",
    "\n",
    "# Flatten the dataset\n",
    "flattened_dataset = emotions[\"train\"].flatten()\n",
    "\n",
    "# Get unique values from a column\n",
    "unique_values = emotions[\"train\"].unique(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4f82f-f5da-44ab-811b-2325cbc2c2ea",
   "metadata": {},
   "source": [
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6584aa99-b209-4b27-80b6-dae380994dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [00:01<00:00, 11728.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to apply to each example\n",
    "def preprocess_example(example):\n",
    "    example[\"text\"] = example[\"text\"].lower()\n",
    "    return example\n",
    "\n",
    "# Apply the function to each example\n",
    "preprocessed_dataset = emotions[\"train\"].map(preprocess_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affcd463-4670-4f68-9537-503e9d276f95",
   "metadata": {},
   "source": [
    "Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c49b4-b9fb-4851-a3e0-45343a2e90f2",
   "metadata": {},
   "source": [
    "The following code snippet is for demonstration purposes only.\n",
    "\n",
    "**Using it will download over 20G of data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "01140360-dd87-4ea8-a356-d365222be474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 312M/312M [00:57<00:00, 5.41MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 312M/312M [00:59<00:00, 5.23MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313M/313M [01:02<00:00, 5.00MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313M/313M [01:02<00:00, 4.99MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 311M/311M [00:55<00:00, 5.59MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 312M/312M [00:57<00:00, 5.47MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313M/313M [00:55<00:00, 5.62MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313M/313M [01:03<00:00, 4.95MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 312M/312M [00:55<00:00, 5.65MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209M/209M [00:38<00:00, 5.48MB/s]\n",
      "Generating train split: 74004228 examples [01:40, 737189.18 examples/s] \n",
      "C:\\Users\\dimit\\miniconda3\\envs\\nlp\\Lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35.9k/35.9k [00:00<00:00, 18.3MB/s]\n",
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.3k/16.3k [00:00<00:00, 8.10MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.3k/15.3k [00:00<00:00, 5.12MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.3G/20.3G [15:31<00:00, 21.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "assert bookcorpus.features.type == wiki.features.type\n",
    "bert_dataset = concatenate_datasets([bookcorpus, wiki])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d6fd8-7f7b-46ad-84da-e0cb6b30e974",
   "metadata": {},
   "source": [
    "Change Format\n",
    "\n",
    "The set_format() function changes the format of a column to be compatible with some common data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "335e543c-7192-41ca-8131-e30aa516510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.set_format(type=\"pandas\")\n",
    "\n",
    "# Restore original format\n",
    "emotions.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f660524-1680-4e79-b004-f26c21ebb685",
   "metadata": {},
   "source": [
    "# Saving and exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2923a-a13b-485c-80ce-710ca5902f9e",
   "metadata": {},
   "source": [
    "You can save and load your dataset locally using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5dad90e7-9e6a-46a6-b27a-3c0169b1160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [00:00<00:00, 1228042.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 165950.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 265882.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "emotions.save_to_disk(\"./\")\n",
    "\n",
    "reloaded_dataset = load_from_disk(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b6a7c-130f-4a36-ba15-e6136fedf571",
   "metadata": {},
   "source": [
    "Export to various data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "75229630-3df9-4a34-aeae-5e6f396bc6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 145.98ba/s]\n",
      "Creating json from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 337.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 559.53ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1741533"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[\"train\"].to_csv(\"./dataset.csv\")\n",
    "emotions[\"train\"].to_json(\"./dataset.json\")\n",
    "emotions[\"train\"].to_parquet(\"./dataset.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
